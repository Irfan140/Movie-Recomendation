{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to end Deep Learning Project Using Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence \n",
    "from tensorflow.keras.models import Sequential # for sequential model\n",
    "from tensorflow.keras.layers import Embedding,SimpleRNN,Dense # for embedding, RNN and Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the imdb dataset (it gets downloaded automatically and is stored in cashe memory) \n",
    "\n",
    "max_features=10000 # vocabulary size\n",
    "# maximum number of words to consider\n",
    "(X_train,y_train),(X_test,y_test)=imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Print the shape of the data\n",
    "print(f'Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}')\n",
    "print(f'Testing data shape: {X_train.shape}, Testing labels shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot representation of the labels\n",
    "X_train[0],y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect a sample review and its label\n",
    "sample_review=X_train[0]\n",
    "sample_label=y_train[0]\n",
    "\n",
    "print(f\"Sample review (as integers):{sample_review}\")\n",
    "print(f'Sample label: {sample_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MApping of words index back to words(for our understanding)\n",
    "word_index=imdb.get_word_index()\n",
    "word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse_word_index is a dictionary that maps the integer index back to the word\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it does not include the first three indices (0, 1, 2) which are reserved for padding, start of sequence and unknown words\n",
    "# We need to add 3 to the indices to get the correct word\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in sample_review])\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_len=500\n",
    "\n",
    "# We will pad the sequences to a maximum length of 500 words\n",
    "# This means that if a review has less than 500 words, it will be padded with zeros at the beginning\n",
    "# If a review has more than 500 words, it will be truncated to 500 words\n",
    "X_train=sequence.pad_sequences(X_train,maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Simple RNN\n",
    "\n",
    "## We will use a simple RNN model with an embedding layer, a simple RNN layer and a dense output layer\n",
    "## The embedding layer will convert the integer indices into dense vectors of fixed size\n",
    "## The simple RNN layer will process the sequences and the dense output layer will give the final output\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_features,128,input_length=max_len)) ## Embedding Layers\n",
    "model.add(SimpleRNN(128,activation='tanh')) # RNNs typically perform better and are more stable with tanh or sigmoid activations inside the recurrent layer.\n",
    "model.add(Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an instance of EarlyStoppping Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystopping=EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
    "earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model with early sstopping\n",
    "history=model.fit(\n",
    "    X_train,y_train,epochs=10,batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model file\n",
    "model.save('simple_rnn_imdb.h5')\n",
    "# or\n",
    "# model.save('my_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
